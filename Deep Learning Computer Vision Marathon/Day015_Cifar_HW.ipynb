{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習內容』\n",
    "#### 運用這幾天所學觀念搭建一個CNN分類器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習目的』\n",
    "  #### 熟悉CNN分類器搭建步驟與原理\n",
    "  #### 學員們可以嘗試不同搭法，如使用不同的Maxpooling層，用GlobalAveragePooling取代Flatten等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import ReLU\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(x_train.shape) #(50000, 32, 32, 3)\n",
    "\n",
    "## Normalize Data\n",
    "def normalize(X_train,X_test):\n",
    "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7) \n",
    "        return X_train, X_test,mean,std\n",
    "    \n",
    "    \n",
    "## Normalize Training and Testset    \n",
    "x_train, x_test,mean_train,std_train = normalize(x_train, x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## OneHot Label 由(None, 1)-(None, 10)\n",
    "## ex. label=2,變成[0,0,1,0,0,0,0,0,0,0]\n",
    "one_hot=OneHotEncoder()\n",
    "y_train=one_hot.fit_transform(y_train).toarray()\n",
    "y_test=one_hot.transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0309 15:12:18.041326 15444 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0309 15:12:18.070223 15444 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0309 15:12:18.075232 15444 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0309 15:12:18.108122 15444 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0309 15:12:18.110117 15444 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0309 15:12:20.730597 15444 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0309 15:12:20.795422 15444 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\ipykernel_launcher.py:58: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
      "W0309 15:12:21.550403 15444 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0309 15:12:21.666861 15444 deprecation.py:323] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 2,502,154\n",
      "Trainable params: 2,499,018\n",
      "Non-trainable params: 3,136\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 22s 432us/step - loss: 1.3089 - acc: 0.5238\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 20s 405us/step - loss: 0.8800 - acc: 0.6882\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 20s 401us/step - loss: 0.6927 - acc: 0.7578\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 22s 431us/step - loss: 0.5750 - acc: 0.8016\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 21s 426us/step - loss: 0.4981 - acc: 0.8268\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 22s 446us/step - loss: 0.4240 - acc: 0.8538\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 22s 442us/step - loss: 0.3663 - acc: 0.87230s - loss: 0.36\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 23s 451us/step - loss: 0.3193 - acc: 0.8882\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 21s 426us/step - loss: 0.2677 - acc: 0.9065\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 21s 421us/step - loss: 0.2236 - acc: 0.9215\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 20s 393us/step - loss: 0.1913 - acc: 0.9316\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 18s 367us/step - loss: 0.1585 - acc: 0.9444\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.1325 - acc: 0.9537\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.1095 - acc: 0.9614\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 20s 409us/step - loss: 0.1017 - acc: 0.9643\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0885 - acc: 0.9692\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 19s 373us/step - loss: 0.0777 - acc: 0.9732\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 19s 372us/step - loss: 0.0748 - acc: 0.9736\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 18s 356us/step - loss: 0.0661 - acc: 0.9767\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 18s 351us/step - loss: 0.0569 - acc: 0.9800\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 18s 352us/step - loss: 0.0646 - acc: 0.9773\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 18s 359us/step - loss: 0.0520 - acc: 0.9824\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 18s 358us/step - loss: 0.0444 - acc: 0.9847\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 18s 359us/step - loss: 0.0557 - acc: 0.9805\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 18s 359us/step - loss: 0.0424 - acc: 0.9853\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0421 - acc: 0.98520s - loss: 0.0419 - acc: 0.\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 18s 361us/step - loss: 0.0452 - acc: 0.9842\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 18s 359us/step - loss: 0.0413 - acc: 0.9855\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 0.0361 - acc: 0.9875\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 18s 362us/step - loss: 0.0345 - acc: 0.9881\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0334 - acc: 0.9887\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 18s 361us/step - loss: 0.0359 - acc: 0.9877\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 18s 361us/step - loss: 0.0328 - acc: 0.9888\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0306 - acc: 0.9889\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 18s 363us/step - loss: 0.0315 - acc: 0.9894\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 18s 362us/step - loss: 0.0282 - acc: 0.9899\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0343 - acc: 0.9883\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 18s 362us/step - loss: 0.0249 - acc: 0.9915\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0215 - acc: 0.9924\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0314 - acc: 0.9895\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0209 - acc: 0.9931\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 18s 363us/step - loss: 0.0276 - acc: 0.9908\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0236 - acc: 0.9919\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 18s 359us/step - loss: 0.0255 - acc: 0.9912\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0227 - acc: 0.9919\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 18s 362us/step - loss: 0.0223 - acc: 0.9926\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0217 - acc: 0.9925\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 18s 359us/step - loss: 0.0216 - acc: 0.9928\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 18s 363us/step - loss: 0.0203 - acc: 0.9932\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0175 - acc: 0.9943\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 18s 359us/step - loss: 0.0236 - acc: 0.9916\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 0.0204 - acc: 0.9931\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 18s 365us/step - loss: 0.0192 - acc: 0.9936\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 18s 355us/step - loss: 0.0178 - acc: 0.9937\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 18s 356us/step - loss: 0.0185 - acc: 0.9936\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 18s 351us/step - loss: 0.0131 - acc: 0.9954\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 18s 351us/step - loss: 0.0187 - acc: 0.9937\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 18s 353us/step - loss: 0.0207 - acc: 0.9928\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 18s 365us/step - loss: 0.0149 - acc: 0.9951\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 18s 368us/step - loss: 0.0133 - acc: 0.9956\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0194 - acc: 0.9933\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0142 - acc: 0.9955\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 0.0178 - acc: 0.9936\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0128 - acc: 0.9956\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 19s 381us/step - loss: 0.0176 - acc: 0.9941\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 20s 403us/step - loss: 0.0134 - acc: 0.9952\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.0158 - acc: 0.9950\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 20s 403us/step - loss: 0.0112 - acc: 0.9958\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 20s 397us/step - loss: 0.0171 - acc: 0.9942\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 20s 390us/step - loss: 0.0135 - acc: 0.9956\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0156 - acc: 0.9951\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0117 - acc: 0.9962\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0124 - acc: 0.9956\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0151 - acc: 0.9949\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0115 - acc: 0.9959\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 19s 370us/step - loss: 0.0149 - acc: 0.9949\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 18s 361us/step - loss: 0.0115 - acc: 0.9960\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 18s 362us/step - loss: 0.0124 - acc: 0.9956\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 18s 361us/step - loss: 0.0150 - acc: 0.9949\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 18s 362us/step - loss: 0.0124 - acc: 0.9959\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 0.0099 - acc: 0.9966\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 18s 362us/step - loss: 0.0080 - acc: 0.9972\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 18s 365us/step - loss: 0.0155 - acc: 0.9946\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0091 - acc: 0.9971\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 18s 351us/step - loss: 0.0127 - acc: 0.9958\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 18s 351us/step - loss: 0.0133 - acc: 0.9958\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 0.0103 - acc: 0.9965\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 18s 363us/step - loss: 0.0059 - acc: 0.9981\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 18s 363us/step - loss: 0.0156 - acc: 0.9949\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 18s 357us/step - loss: 0.0073 - acc: 0.9971\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 0.0118 - acc: 0.9963\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 18s 361us/step - loss: 0.0090 - acc: 0.9968\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 18s 353us/step - loss: 0.0091 - acc: 0.9969\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 18s 361us/step - loss: 0.0115 - acc: 0.9960\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 18s 351us/step - loss: 0.0089 - acc: 0.9971\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 18s 357us/step - loss: 0.0114 - acc: 0.9966\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 0.0104 - acc: 0.9963\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 18s 363us/step - loss: 0.0102 - acc: 0.9967\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 18s 360us/step - loss: 0.0093 - acc: 0.9969\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 18s 362us/step - loss: 0.0069 - acc: 0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23100eb12e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=Sequential()\n",
    "\n",
    "#卷積組合\n",
    "classifier.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=(32,32,3)))#32,3,3,input_shape=(32,32,3),activation='relu''\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "\n",
    "'''自己決定MaxPooling2D放在哪裡'''\n",
    "#classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#卷積組合\n",
    "classifier.add(Conv2D(filters=64, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(Conv2D(filters=64, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "\n",
    "\n",
    "classifier.add(Conv2D(filters=128, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(Conv2D(filters=128, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(Conv2D(filters=128, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "classifier.add(Conv2D(filters=256, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(Conv2D(filters=256, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(Conv2D(filters=512, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "#flatten\n",
    "#classifier.add(Flatten())\n",
    "classifier.add(GlobalAveragePooling2D())\n",
    "\n",
    "#FC\n",
    "#classifier.add(Dense(units= 1024, activation='relu')) #output_dim=100,activation=relu\n",
    "\n",
    "#輸出\n",
    "classifier.add(Dense(output_dim=10,activation='softmax'))\n",
    "classifier.summary()\n",
    "\n",
    "#超過兩個就要選categorical_crossentrophy\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(x_train,y_train,batch_size=64,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測新圖片，輸入影像前處理要與訓練時相同\n",
    "#### ((X-mean)/(std+1e-7) ):這裡的mean跟std是訓練集的\n",
    "## 維度如下方示範"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 282us/step\n",
      "Loss :     0.939218\n",
      "Accuracy : 0.855900\n"
     ]
    }
   ],
   "source": [
    "# input_example=(np.zeros(shape=(1,32,32,3))-mean_train)/(std_train+1e-7) \n",
    "# classifier.predict(input_example)\n",
    "\n",
    "loss, acc = classifier.evaluate(x_test, y_test)\n",
    "print(\"Loss :     %f\"%loss)\n",
    "print(\"Accuracy : %f\"%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
